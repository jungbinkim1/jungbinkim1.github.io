---
title: Publications
layout: default
nav_order: 3
---

1. Accelerated gradient methods for geodesically convex optimization: Tractable algorithms and convergence analysis (with Insoon Yang). [ICML 2022.](https://proceedings.mlr.press/v162/kim22k/kim22k.pdf) [arXiv.](https://arxiv.org/abs/2202.02036) <br/> <small> Note: Errors in Eq. 6 and Thm. 5.6 have been corrected in the arXiv version (thanks to the suggestion by Prof. Ken'ichiro Tanaka) </small>
1. Unifying Nesterovâ€™s accelerated gradient methods for convex and strongly convex objective functions (with Insoon Yang). [ICML 2023 (Oral).](https://proceedings.mlr.press/v202/kim23y/kim23y.pdf) <br/> <small> Note: If you cite "Unified AGM ODE" or "Unified Bregman Lagrangian flow" in this paper, I encourage you to also cite "NAG flow" in [[Luo & Chen, 2021]](https://arxiv.org/abs/1909.03145), which is essentially the same continuous-time model (but derived using a different approach); see Appendix A.3. </small>
1. Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels (with Insoon Yang). [NeurIPS 2023.](https://proceedings.neurips.cc/paper_files/paper/2023/file/c70741145c2c4f1d0c2e91b98729a49a-Paper-Conference.pdf) 
1. A proof of the exact convergence rate of gradient descent. [arXiv.](https://arxiv.org/pdf/2412.04427)
1. Horospherically Convex Optimization on Hadamard Manifolds Part I: Analysis and Algorithms (with Christopher Criscitiello). [arXiv.](https://arxiv.org/pdf/2505.16970)
