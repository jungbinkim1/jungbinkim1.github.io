---
title: Publications
layout: default
nav_order: 3
---

1. Accelerated gradient methods for geodesically convex optimization: Tractable algorithms and convergence analysis (with Insoon Yang). [ICML 2022.](https://proceedings.mlr.press/v162/kim22k/kim22k.pdf)
1. Unifying Nesterovâ€™s accelerated gradient methods for convex and strongly convex objective functions (with Insoon Yang). [ICML 2023 (Oral).](https://proceedings.mlr.press/v202/kim23y/kim23y.pdf)
1. Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels (with Insoon Yang). [NeurIPS 2023.](https://proceedings.neurips.cc/paper_files/paper/2023/file/c70741145c2c4f1d0c2e91b98729a49a-Paper-Conference.pdf) 
1. A proof of exact convergence rate of gradient descent. Part I. Performance criterion $\|\nabla f(x_N)\|^2/(f(x_0)-f_*)$. [arXiv.](https://arxiv.org/pdf/2412.04435)
1. A proof of exact convergence rate of gradient descent. Part II. Performance criterion $(f(x_N)-f_*)/\|x_0-x_*\|^2$. [arXiv.](https://arxiv.org/pdf/2412.04427)
1. Horospherially convex optimization on Hadamard manifolds. Part I (with Christopher Criscitiello). TBA.
