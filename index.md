---
title: Home
layout: home
---

Jungbin Kim

[Email](mailto:kjb2952@snu.ac.kr), [Google Scholar](https://scholar.google.com/citations?user=8hNmEzMAAAAJ), [GitHub](https://github.com/jungbinkim1)

1. Accelerated gradient methods for geodesically convex optimization: Tractable algorithms and convergence analysis (with Insoon Yang). [ICML 2022.](https://proceedings.mlr.press/v162/kim22k/kim22k.pdf) [arXiv.](https://arxiv.org/abs/2202.02036) <br/> <small> Note: Errors in Eq. 6 and Thm. 5.6 have been corrected in the arXiv version (thanks to the suggestion by Prof. Ken'ichiro Tanaka. </small>
1. Unifying Nesterovâ€™s accelerated gradient methods for convex and strongly convex objective functions (with Insoon Yang). [ICML 2023 (Oral).](https://proceedings.mlr.press/v202/kim23y/kim23y.pdf) <br/> <small> Note: If you cite "Unified AGM ODE" or "Unified Bregman Lagrangian flow" in this paper, please consider also citing "NAG flow" in [[Luo & Chen, 2021, Section 3]](https://arxiv.org/abs/1909.03145), a continuous-time model based on essentially the same idea; see Appendix A.3 of our paper. </small>
1. Convergence analysis of ODE models for accelerated first-order methods via positive semidefinite kernels (with Insoon Yang). [NeurIPS 2023.](https://proceedings.neurips.cc/paper_files/paper/2023/file/c70741145c2c4f1d0c2e91b98729a49a-Paper-Conference.pdf) 
1. A proof of the exact convergence rate of gradient descent. [arXiv.](https://arxiv.org/pdf/2412.04427)
1. Horospherically Convex Optimization on Hadamard Manifolds Part I: Analysis and Algorithms (with Christopher Criscitiello). [arXiv.](https://arxiv.org/pdf/2505.16970)
